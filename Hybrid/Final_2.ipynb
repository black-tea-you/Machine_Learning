{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ede70b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask_ml.recommendation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdms\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client, LocalCluster\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecommendation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ALS\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dask_ml.recommendation'"
     ]
    }
   ],
   "source": [
    "# (MODIFIED CELL 2) - Dask Import & Setup\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask_ml.cluster as dkm\n",
    "import dask_ml.decomposition as dkd\n",
    "import dask_ml.preprocessing as dpr\n",
    "import dask_ml.model_selection as dms\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import xgboost as xgb # Dask-XGBoostëŠ” dask.dataframeì„ ì§ì ‘ ë°›ìŒ\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder # LabelEncoderëŠ” Dask-MLì— ì—†ìŒ (ëŒ€ì²´ í•„ìš”)\n",
    "\n",
    "# --- 0. í•˜ì´í¼íŒŒë¼ë¯¸í„° (Daskìš©) ---\n",
    "# (SAMPLE_FRACì€ 1.0ì´ë¯€ë¡œ ì‚­ì œ)\n",
    "SOURCE_FILE = 'df_master_preprocessed.parquet'\n",
    "K_BEERS = 6\n",
    "K_USERS = 8\n",
    "N_COMPONENTS = 10 # Latent Factor ê°œìˆ˜\n",
    "THRESHOLD = 0.5   # (ì´ì „ì— ì¡°ì •í•œ ê°’)\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Dask ë° Dask-ML ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72033680",
   "metadata": {},
   "source": [
    "# (MODIFIED CELL 2) - Dask Client Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32e23169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask Clientê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "Dask ëŒ€ì‹œë³´ë“œ ë§í¬: http://127.0.0.1:56804/status\n",
      "ìœ„ ë§í¬ë¥¼ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ Dask ì‘ì—… ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# (MODIFIED CELL 2) - Dask Client Start\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster, wait # (â˜…â˜…â˜…â˜…â˜… wait ì„í¬íŠ¸ ì¶”ê°€ â˜…â˜…â˜…â˜…â˜…)\n",
    "import warnings\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dask í´ëŸ¬ìŠ¤í„°ë¥¼ ë¡œì»¬ ë¨¸ì‹ ì— ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "# (n_workers, memory_limit ë“±ì€ ë¨¸ì‹  ì‚¬ì–‘ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”)\n",
    "try:\n",
    "    cluster = LocalCluster(n_workers=4, threads_per_worker=1, memory_limit='16GB')\n",
    "    client = Client(cluster)\n",
    "    print(\"Dask Clientê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"Dask ëŒ€ì‹œë³´ë“œ ë§í¬: {client.dashboard_link}\")\n",
    "    print(\"ìœ„ ë§í¬ë¥¼ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ Dask ì‘ì—… ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.\")\n",
    "except Exception as e:\n",
    "    print(f\"Dask í´ë¼ì´ì–¸íŠ¸ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"Daskê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ---\n",
    "SOURCE_FILE = 'df_master_preprocessed.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9186907",
   "metadata": {},
   "source": [
    "# (MODIFIED CELL 3) - 840ë§Œ í–‰ Daskë¡œ ë¡œë“œ (ìƒ˜í”Œë§ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d698be5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. ì „ì²´ ë°ì´í„° ë¡œë“œ (df_master_preprocessed.parquet) ---\n",
      "ì „ì²´ ë°ì´í„° ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ. (Dask lazy loading)\n",
      "ì „ì²´ ë°ì´í„° í–‰ ìˆ˜ (ì˜ˆìƒ): 8417033\n"
     ]
    }
   ],
   "source": [
    "# (MODIFIED CELL 3) - 840ë§Œ í–‰ Daskë¡œ ë¡œë“œ (ìƒ˜í”Œë§ X)\n",
    "\n",
    "print(f\"--- 1. ì „ì²´ ë°ì´í„° ë¡œë“œ ({SOURCE_FILE}) ---\")\n",
    "# (â˜…ì¤‘ìš”â˜…) 840ë§Œ í–‰ ì „ì²´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (ì•„ì§ ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°€ì§€ ì•ŠìŒ)\n",
    "# engine='pyarrow'ê°€ parquet ì²˜ë¦¬ì— íš¨ìœ¨ì ì…ë‹ˆë‹¤.\n",
    "# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì§€ì •í•˜ë©´ ì†ë„ê°€ ë” ë¹¨ë¼ì§‘ë‹ˆë‹¤.\n",
    "cols_to_load = ['date', 'style', 'country_brewery', 'abv', 'smell', 'taste', \n",
    "                'feel', 'score', 'username', 'beer_id']\n",
    "\n",
    "try:\n",
    "    df_full = dd.read_parquet(\n",
    "        SOURCE_FILE, \n",
    "        engine='pyarrow',\n",
    "        columns=cols_to_load \n",
    "    )\n",
    "    \n",
    "    # npartitionsë¥¼ ì¡°ì •í•˜ì—¬ ì²­í¬ í¬ê¸° ë³€ê²½ ê°€ëŠ¥ (ì˜ˆ: repartition(npartitions=50))\n",
    "    # df_full = df_full.repartition(npartitions=50) # (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\n",
    "\n",
    "    # Daskê°€ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ë‘ê³  ì¬ì‚¬ìš©í•˜ë„ë¡ ì§€ì‹œ (ê³„ì‚° ì†ë„ í–¥ìƒ)\n",
    "    df_full = df_full.persist() \n",
    "    \n",
    "    print(f\"ì „ì²´ ë°ì´í„° ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ. (Dask lazy loading)\")\n",
    "    print(f\"ì „ì²´ ë°ì´í„° í–‰ ìˆ˜ (ì˜ˆìƒ): {len(df_full)}\") # .compute() ì „ì´ë¼ë„ len()ì€ ë¹ ë¥´ê²Œ ê³„ì‚°ë¨\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Daskë¡œ Parquet ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"Dask ë˜ëŠ” pyarrow ì„¤ì¹˜, íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d11a8a",
   "metadata": {},
   "source": [
    "# (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acdc9702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 'style_group' ë° 'geo_group' í”¼ì²˜ ìƒì„± ì¤‘ (Dask) ---\n",
      "'style_group', 'geo_group' ìƒì„± ì™„ë£Œ (Lazy).\n"
     ]
    }
   ],
   "source": [
    "# (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "\n",
    "print(\"\\n--- 'style_group' ë° 'geo_group' í”¼ì²˜ ìƒì„± ì¤‘ (Dask) ---\")\n",
    "\n",
    "def group_style(style):\n",
    "    if 'IPA' in str(style): return 'IPA'\n",
    "    if 'Stout' in str(style): return 'Stout'\n",
    "    if 'Ale' in str(style): return 'Ale'\n",
    "    return 'Other'\n",
    "    \n",
    "def group_country(country):\n",
    "    if country == 'US': return 'US'\n",
    "    if country in ['DE', 'GB', 'BE']: return 'Europe'\n",
    "    return 'Other'\n",
    "\n",
    "# (â˜…ìˆ˜ì •â˜…) .apply -> .map_partitions\n",
    "# DaskëŠ” map_partitionsë¥¼ ì‚¬ìš©í•  ë•Œ ì¶œë ¥ ê²°ê³¼ì˜ íƒ€ì…(meta)ì„ ì•Œë ¤ì¤˜ì•¼ í•©ë‹ˆë‹¤.\n",
    "df_full['style_group'] = df_full['style'].map_partitions(\n",
    "    lambda s: s.apply(group_style), \n",
    "    meta=pd.Series(dtype='object', name='style_group')\n",
    ")\n",
    "df_full['geo_group'] = df_full['country_brewery'].map_partitions(\n",
    "    lambda s: s.apply(group_country), \n",
    "    meta=pd.Series(dtype='object', name='geo_group')\n",
    ")\n",
    "\n",
    "print(\"'style_group', 'geo_group' ìƒì„± ì™„ë£Œ (Lazy).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78da386",
   "metadata": {},
   "source": [
    "# (MODIFIED CELL 5) - Dask Load Test (wait í•¨ìˆ˜ ìˆ˜ì •)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ce80a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Daskë¡œ ì „ì²´ ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸ (df_master_preprocessed.parquet) ---\n",
      "íŒŒì¼ ì½ê¸° ì¤€ë¹„ ì™„ë£Œ. (Lazy Loading)\n",
      "Dask .persist() ì‹œì‘... (ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤. ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n",
      "--- ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ---\n",
      "ë¡œë“œ ì‹œê°„: 1.57 ì´ˆ\n",
      "ì´ 8417033 í–‰ì´ Dask ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# (MODIFIED CELL 5) - Dask Load Test (wait í•¨ìˆ˜ ìˆ˜ì •)\n",
    "\n",
    "import time \n",
    "\n",
    "print(f\"--- 1. Daskë¡œ ì „ì²´ ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸ ({SOURCE_FILE}) ---\")\n",
    "\n",
    "if 'client' not in locals():\n",
    "    print(\"!!! ì˜¤ë¥˜: Dask Clientê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì´ì „ ì…€ ì‹¤í–‰ í•„ìš”)\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. 840ë§Œ í–‰ ì „ì²´ë¥¼ Daskë¡œ ë¡œë“œ (Lazy)\n",
    "        cols_to_load = ['username', 'beer_id', 'score', 'date', 'style', 'country_brewery']\n",
    "        \n",
    "        df_full = dd.read_parquet(\n",
    "            SOURCE_FILE, \n",
    "            engine='pyarrow',\n",
    "            columns=cols_to_load\n",
    "        )\n",
    "        \n",
    "        print(f\"íŒŒì¼ ì½ê¸° ì¤€ë¹„ ì™„ë£Œ. (Lazy Loading)\")\n",
    "        \n",
    "        # 2. (â˜…í•µì‹¬ í…ŒìŠ¤íŠ¸â˜…) .persist()\n",
    "        print(\"Dask .persist() ì‹œì‘... (ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤. ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        df_full = df_full.persist() # (â˜…ê³„ì‚° ë°œìƒâ˜…)\n",
    "        \n",
    "        # 3. (â˜…â˜…â˜…â˜…â˜… ì˜¤ë¥˜ ìˆ˜ì • â˜…â˜…â˜…â˜…â˜…)\n",
    "        # .persist()ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸° (client.wait_for_futures -> wait)\n",
    "        wait(df_full) \n",
    "        \n",
    "        total_rows = len(df_full)\n",
    "        \n",
    "        print(f\"--- ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ---\")\n",
    "        print(f\"ë¡œë“œ ì‹œê°„: {(time.time() - start_time):.2f} ì´ˆ\")\n",
    "        print(f\"ì´ {total_rows} í–‰ì´ Dask ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"!!! ì˜¤ë¥˜: '{SOURCE_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Dask ë¡œë“œ(.persist()) ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"ì´ì œ 'time' ë˜ëŠ” 'wait' ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¼ë©´, ì´ê²ƒì´ ì§„ì§œ ë©”ëª¨ë¦¬ ë¶€ì¡±/pyarrow ì˜¤ë¥˜ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e4d4c",
   "metadata": {},
   "source": [
    "# (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "226a4661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 'style_group' ë° 'geo_group' í”¼ì²˜ ìƒì„± ì¤‘ (Dask) ---\n",
      "'style_group', 'geo_group' ìƒì„± ë° Persist ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "# (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "\n",
    "print(\"\\n--- 'style_group' ë° 'geo_group' í”¼ì²˜ ìƒì„± ì¤‘ (Dask) ---\")\n",
    "\n",
    "# (df_fullì´ ì´ì „ Cell 5 í…ŒìŠ¤íŠ¸ì—ì„œ ë¡œë“œë˜ì—ˆë‹¤ê³  ê°€ì •)\n",
    "if 'df_full' not in locals() or not isinstance(df_full, dd.DataFrame):\n",
    "    print(\"!!! ì˜¤ë¥˜: df_full (Dask DataFrame)ì´ ì—†ìŠµë‹ˆë‹¤. ë¡œë“œ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    def group_style(style):\n",
    "        # (styleì€ NAê°€ ìˆì–´ë„ 'IPA' in 'NA' (False)ê°€ ë˜ì–´ ì•ˆì „í•¨)\n",
    "        if 'IPA' in str(style): return 'IPA'\n",
    "        if 'Stout' in str(style): return 'Stout'\n",
    "        if 'Ale' in str(style): return 'Ale'\n",
    "        return 'Other'\n",
    "        \n",
    "    def group_country(country):\n",
    "        # --- ğŸ’¡ [ìˆ˜ì •ëœ ë¶€ë¶„] ---\n",
    "        # \"boolean value of NA\" ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ NAë¥¼ ë¨¼ì € ì²´í¬í•©ë‹ˆë‹¤.\n",
    "        if pd.isna(country):\n",
    "            return 'Other' # ë˜ëŠ” return pd.NA\n",
    "        # ---\n",
    "        if country == 'US': return 'US'\n",
    "        if country in ['DE', 'GB', 'BE']: return 'Europe'\n",
    "        return 'Other'\n",
    "\n",
    "    # (â˜…ìˆ˜ì •â˜…) .apply -> .map_partitions\n",
    "    df_full['style_group'] = df_full['style'].map_partitions(\n",
    "        lambda s: s.apply(group_style), \n",
    "        meta=pd.Series(dtype='object', name='style_group')\n",
    "    )\n",
    "    df_full['geo_group'] = df_full['country_brewery'].map_partitions(\n",
    "        lambda s: s.apply(group_country), \n",
    "        meta=pd.Series(dtype='object', name='geo_group')\n",
    "    )\n",
    "    \n",
    "    # (â˜…í•µì‹¬â˜…) .persist()ë¡œ CB í”¼ì²˜ ê³„ì‚° ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€\n",
    "    df_full = df_full.persist()\n",
    "    wait(df_full) # ê³„ì‚° ì™„ë£Œê¹Œì§€ ëŒ€ê¸°\n",
    "    \n",
    "    print(\"'style_group', 'geo_group' ìƒì„± ë° Persist ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738cec36",
   "metadata": {},
   "source": [
    "# (MODIFIED CELL 5) - Latent Feature (SVD) (Dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32900605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3-A. Latent Feature ì¶”ì¶œ (SVD) (Dask) ---\n",
      "í”¼ë²— ëŒ€ìƒ ì»¬ëŸ¼('username', 'beer_id', 'score')ì˜ ê²°ì¸¡ì¹˜(NA)ë¥¼ ì œê±°í•©ë‹ˆë‹¤...\n",
      "ê²°ì¸¡ì¹˜ ì œê±° ì™„ë£Œ (Lazy).\n",
      "Warning: 840ë§Œ í–‰ì— ëŒ€í•œ categorize() ë° pivot_tableì€\n",
      "         ë§¤ìš° ëŠë¦¬ê³  Dask í´ëŸ¬ìŠ¤í„° ë©”ëª¨ë¦¬ë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "Pivotingì„ ìœ„í•´ 'username'ê³¼ 'beer_id'ë¥¼ category íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤...\n",
      "ì´ ì‘ì—…ì€ ê³ ìœ ê°’(cardinality)ì´ ë§ì•„ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "Category íƒ€ì… ë³€í™˜ ì™„ë£Œ (Known). Pivot ì‹œì‘...\n",
      "Pivot í…Œì´ë¸” ìƒì„± ì™„ë£Œ. SVDë¥¼ ìœ„í•´ NaNì„ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤...\n",
      "NaN ì±„ìš°ê¸° ì™„ë£Œ(Lazy). SVD ê³„ì‚° ì‹œì‘...\n",
      "SVD .fit_transform() ê³„ì‚° ì‹œì‘ (Dask Array ì‚¬ìš©)...\n",
      "!!! Dask SVD/Pivot ì¤‘ ì˜¤ë¥˜ ë°œìƒ: Unable to allocate 21.1 GiB for an array with shape (5660664730,) and data type int32\n",
      "--- 'category dtype' ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¼ë©´, ì´ê²ƒì´ ì§„ì§œ ë©”ëª¨ë¦¬/ì„±ëŠ¥ í•œê³„ì…ë‹ˆë‹¤. ---\n"
     ]
    }
   ],
   "source": [
    "# --- 3-A. Latent Feature ì¶”ì¶œ (SVD) (Dask) ---\n",
    "print(\"--- 3-A. Latent Feature ì¶”ì¶œ (SVD) (Dask) ---\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Dask DataFrame ì¤€ë¹„ ---\n",
    "    if 'df_full' not in locals():\n",
    "         raise NameError(\"df_full (Dask DataFrame)ì´ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ë¡œë“œ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # df_fullì„ ddf ë³€ìˆ˜ëª…ìœ¼ë¡œ ë°›ì•„ì„œ SVD ì „ìš©ìœ¼ë¡œ ì‚¬ìš©\n",
    "    ddf = df_full \n",
    "    \n",
    "    # --- 2. NA/ê²°ì¸¡ì¹˜ ì œê±° ---\n",
    "    # í”¼ë²—ì— ì‚¬ìš©í•  í•µì‹¬ ì»¬ëŸ¼ 3ê°œ('username', 'beer_id', 'score')ì˜ NAë¥¼ ì œê±°\n",
    "    print(\"í”¼ë²— ëŒ€ìƒ ì»¬ëŸ¼('username', 'beer_id', 'score')ì˜ ê²°ì¸¡ì¹˜(NA)ë¥¼ ì œê±°í•©ë‹ˆë‹¤...\")\n",
    "    ddf = ddf.dropna(subset=['username', 'beer_id', 'score'])\n",
    "    print(\"ê²°ì¸¡ì¹˜ ì œê±° ì™„ë£Œ (Lazy).\")\n",
    "\n",
    "    # --- 3. categorize() ì‹¤í–‰ ---\n",
    "    print(\"Warning: 840ë§Œ í–‰ì— ëŒ€í•œ categorize() ë° pivot_tableì€\")\n",
    "    print(\"         ë§¤ìš° ëŠë¦¬ê³  Dask í´ëŸ¬ìŠ¤í„° ë©”ëª¨ë¦¬ë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    print(\"Pivotingì„ ìœ„í•´ 'username'ê³¼ 'beer_id'ë¥¼ category íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤...\")\n",
    "    print(\"ì´ ì‘ì—…ì€ ê³ ìœ ê°’(cardinality)ì´ ë§ì•„ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    ddf = ddf.categorize(columns=['username', 'beer_id'])\n",
    "    \n",
    "    # (ì„ íƒ ì‚¬í•­) ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë‹¤ë©´ persist() ì‚¬ìš©\n",
    "    # ddf = ddf.persist() \n",
    "\n",
    "    print(\"Category íƒ€ì… ë³€í™˜ ì™„ë£Œ (Known). Pivot ì‹œì‘...\")\n",
    "\n",
    "    # --- 4. pivot_table ì‹¤í–‰ ('score' ì‚¬ìš©) ---\n",
    "    pivot_df = ddf.pivot_table(index='username', \n",
    "                               columns='beer_id', \n",
    "                               values='score') \n",
    "\n",
    "    # --- 5. SVDë¥¼ ìœ„í•œ .fillna(0) ---\n",
    "    print(\"Pivot í…Œì´ë¸” ìƒì„± ì™„ë£Œ. SVDë¥¼ ìœ„í•´ NaNì„ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤...\")\n",
    "    pivot_df_filled = pivot_df.fillna(0)\n",
    "    print(\"NaN ì±„ìš°ê¸° ì™„ë£Œ(Lazy). SVD ê³„ì‚° ì‹œì‘...\")\n",
    "    \n",
    "    # --- ğŸ’¡ 6. [ìˆ˜ì •ëœ ë¶€ë¶„] SVD ë¡œì§ ì‹¤í–‰ ---\n",
    "    # (N_COMPONENTSëŠ” Cell 37ì—ì„œ 10ìœ¼ë¡œ ì •ì˜ë¨)\n",
    "    svd = dkd.TruncatedSVD(n_components=N_COMPONENTS, random_state=42)\n",
    "    \n",
    "    print(\"SVD .fit_transform() ê³„ì‚° ì‹œì‘ (Dask Array ì‚¬ìš©)...\")\n",
    "    \n",
    "    # \n",
    "    # (ğŸ’¡ğŸ’¡ğŸ’¡ ì—¬ê¸°ê°€ ìˆ˜ì •ëœ ë¶€ë¶„ì…ë‹ˆë‹¤ ğŸ’¡ğŸ’¡ğŸ’¡)\n",
    "    # 'numblocks' ì˜¤ë¥˜ í•´ê²°: .valuesë¥¼ ë¶™ì—¬ Dask Arrayë¡œ ë³€í™˜\n",
    "    #\n",
    "    user_latent_features = svd.fit_transform(pivot_df_filled.values) # <--- .values ì¶”ê°€\n",
    "    beer_latent_features = svd.components_\n",
    "    \n",
    "    # (SVD ê²°ê³¼ë¥¼ ë‹¤ë¥¸ ì…€ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ DataFrameìœ¼ë¡œ ë³€í™˜)\n",
    "    user_latent_df = pivot_df_filled.index.to_frame(name='username')\\\n",
    "                       .assign(**{f'user_latent_{i}': user_latent_features[:, i] for i in range(N_COMPONENTS)})\n",
    "    beer_latent_df = pivot_df_filled.columns.to_frame(name='beer_id')\\\n",
    "                       .assign(**{f'beer_latent_{i}': beer_latent_features[i, :] for i in range(N_COMPONENTS)})\n",
    "\n",
    "    # (ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ê³ ì •)\n",
    "    user_latent_df = user_latent_df.persist()\n",
    "    beer_latent_df = beer_latent_df.persist()\n",
    "    wait(user_latent_df)\n",
    "    wait(beer_latent_df)\n",
    "    \n",
    "    print(\"--- Dask SVD/Pivot ëª¨ë“  ì‘ì—… ì™„ë£Œ ---\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!!! Dask SVD/Pivot ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"--- 'category dtype' ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¼ë©´, ì´ê²ƒì´ ì§„ì§œ ë©”ëª¨ë¦¬/ì„±ëŠ¥ í•œê³„ì…ë‹ˆë‹¤. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8419799",
   "metadata": {},
   "source": [
    "# (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§???????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5a2d759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 'style_group' ë° 'geo_group' í”¼ì²˜ ìƒì„± ì¤‘ (Dask) ---\n",
      "'style_group', 'geo_group' ìƒì„± ë° Persist ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "# (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "\n",
    "print(\"\\n--- 'style_group' ë° 'geo_group' í”¼ì²˜ ìƒì„± ì¤‘ (Dask) ---\")\n",
    "\n",
    "# (df_fullì´ ì´ì „ Cell 5 í…ŒìŠ¤íŠ¸ì—ì„œ ë¡œë“œë˜ì—ˆë‹¤ê³  ê°€ì •)\n",
    "if 'df_full' not in locals() or not isinstance(df_full, dd.DataFrame):\n",
    "    print(\"!!! ì˜¤ë¥˜: df_full (Dask DataFrame)ì´ ì—†ìŠµë‹ˆë‹¤. ë¡œë“œ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    def group_style(style):\n",
    "        if 'IPA' in str(style): return 'IPA'\n",
    "        if 'Stout' in str(style): return 'Stout'\n",
    "        if 'Ale' in str(style): return 'Ale'\n",
    "        return 'Other'\n",
    "        \n",
    "    def group_country(country):\n",
    "        if country == 'US': return 'US'\n",
    "        if country in ['DE', 'GB', 'BE']: return 'Europe'\n",
    "        return 'Other'\n",
    "\n",
    "    # (â˜…ìˆ˜ì •â˜…) .apply -> .map_partitions\n",
    "    df_full['style_group'] = df_full['style'].map_partitions(\n",
    "        lambda s: s.apply(group_style), \n",
    "        meta=pd.Series(dtype='object', name='style_group')\n",
    "    )\n",
    "    df_full['geo_group'] = df_full['country_brewery'].map_partitions(\n",
    "        lambda s: s.apply(group_country), \n",
    "        meta=pd.Series(dtype='object', name='geo_group')\n",
    "    )\n",
    "    \n",
    "    # (â˜…í•µì‹¬â˜…) .persist()ë¡œ CB í”¼ì²˜ ê³„ì‚° ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€\n",
    "    df_full = df_full.persist()\n",
    "    wait(df_full) # ê³„ì‚° ì™„ë£Œê¹Œì§€ ëŒ€ê¸°\n",
    "    \n",
    "    print(\"'style_group', 'geo_group' ìƒì„± ë° Persist ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bf173e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Beer Clustering (Dask-ML) ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "boolean value of NA is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 15\u001b[0m\n\u001b[0;32m      6\u001b[0m beer_features_df \u001b[38;5;241m=\u001b[39m df_full[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeer_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle_group\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeo_group\u001b[39m\u001b[38;5;124m'\u001b[39m]]\\\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeer_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\\\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mdropna()\\\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeer_id\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# set_indexëŠ” Daskì—ì„œ ëŠë¦° ì‘ì—…\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 2. Dask-ML OneHotEncoding (pd.get_dummies ëŒ€ì²´)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# (Dask-MLì€ ë¬¸ìì—´ ì¹´í…Œê³ ë¦¬ë¥¼ ìˆ«ìë¡œ ë¨¼ì € ë³€í™˜í•´ì•¼ í•  ìˆ˜ ìˆìŒ)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# (ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ get_dummiesë¥¼ ì“°ì§€ë§Œ, ëŒ€ìš©ëŸ‰ì—ì„œëŠ” Categorizer + OneHotEncoder ì¶”ì²œ)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m beer_features_processed \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mget_dummies(\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mbeer_features_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstyle_group\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeo_group\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     16\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle_group\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeo_group\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 3. Dask-ML K-Means\u001b[39;00m\n\u001b[0;32m     20\u001b[0m kmeans_beer \u001b[38;5;241m=\u001b[39m dkm\u001b[38;5;241m.\u001b[39mKMeans(n_clusters\u001b[38;5;241m=\u001b[39mK_BEERS, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:3805\u001b[0m, in \u001b[0;36mDataFrame.categorize\u001b[1;34m(self, columns, index, split_every, **kwargs)\u001b[0m\n\u001b[0;32m   3800\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdask_expr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_collection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m new_collection\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;66;03m# Eagerly compute the categories\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m categories, index \u001b[38;5;241m=\u001b[39m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGetCategories\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m-> 3805\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3807\u001b[0m \u001b[38;5;66;03m# Some operations like get_dummies() rely on the order of categories\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m categories \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39msort_values() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m categories\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:373\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:681\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[0;32m    679\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(flatten(expr\u001b[38;5;241m.\u001b[39m__dask_keys__()))\n\u001b[1;32m--> 681\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(expr, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "Cell \u001b[1;32mIn[26], line 26\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (â˜…ìˆ˜ì •â˜…) .apply -> .map_partitions\u001b[39;00m\n\u001b[0;32m     21\u001b[0m df_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle_group\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap_partitions(\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m s: s\u001b[38;5;241m.\u001b[39mapply(group_style), \n\u001b[0;32m     23\u001b[0m     meta\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mSeries(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle_group\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m df_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeo_group\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_brewery\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap_partitions(\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m s: s\u001b[38;5;241m.\u001b[39mapply(group_country), \n\u001b[0;32m     27\u001b[0m     meta\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mSeries(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeo_group\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# (â˜…í•µì‹¬â˜…) .persist()ë¡œ CB í”¼ì²˜ ê³„ì‚° ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€\u001b[39;00m\n\u001b[0;32m     31\u001b[0m df_full \u001b[38;5;241m=\u001b[39m df_full\u001b[38;5;241m.\u001b[39mpersist()\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[26], line 16\u001b[0m, in \u001b[0;36mgroup_country\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgroup_country\u001b[39m(country):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m country \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUS\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUS\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBE\u001b[39m\u001b[38;5;124m'\u001b[39m]: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEurope\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOther\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mmissing.pyx:392\u001b[0m, in \u001b[0;36mpandas._libs.missing.NAType.__bool__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: boolean value of NA is ambiguous"
     ]
    }
   ],
   "source": [
    "# (MODIFIED CELL 6) - Beer Clustering (Dask-ML)\n",
    "\n",
    "print(\"\\n--- Beer Clustering (Dask-ML) ---\")\n",
    "\n",
    "# 1. ì¬ë£Œ ì¤€ë¹„\n",
    "beer_features_df = df_full[['beer_id', 'style_group', 'geo_group']]\\\n",
    "    .drop_duplicates(subset=['beer_id'])\\\n",
    "    .dropna()\\\n",
    "    .set_index('beer_id') # set_indexëŠ” Daskì—ì„œ ëŠë¦° ì‘ì—…\n",
    "\n",
    "# 2. Dask-ML OneHotEncoding (pd.get_dummies ëŒ€ì²´)\n",
    "# (Dask-MLì€ ë¬¸ìì—´ ì¹´í…Œê³ ë¦¬ë¥¼ ìˆ«ìë¡œ ë¨¼ì € ë³€í™˜í•´ì•¼ í•  ìˆ˜ ìˆìŒ)\n",
    "# (ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ get_dummiesë¥¼ ì“°ì§€ë§Œ, ëŒ€ìš©ëŸ‰ì—ì„œëŠ” Categorizer + OneHotEncoder ì¶”ì²œ)\n",
    "beer_features_processed = dd.get_dummies(\n",
    "    beer_features_df.categorize(columns=['style_group', 'geo_group']),\n",
    "    columns=['style_group', 'geo_group']\n",
    ")\n",
    "\n",
    "# 3. Dask-ML K-Means\n",
    "kmeans_beer = dkm.KMeans(n_clusters=K_BEERS, random_state=42, n_init=10)\n",
    "print(\"Beer K-Means í•™ìŠµ ì‹œì‘ (Dask)...\")\n",
    "\n",
    "# (â˜…ê³„ì‚° ë°œìƒâ˜…) .fit_predict()ëŠ” ê³„ì‚°ì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤ (ë©”ëª¨ë¦¬ ì£¼ì˜)\n",
    "beer_features_df['beer_cluster'] = kmeans_beer.fit_predict(beer_features_processed)\n",
    "beer_features_df = beer_features_df.reset_index() # Mergeë¥¼ ìœ„í•´ ì¸ë±ìŠ¤ ë¦¬ì…‹\n",
    "\n",
    "print(f\"Beer í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ. {K_BEERS}ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜ë¨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MODIFIED CELL 7) - User Clustering (Dask-ML)\n",
    "\n",
    "print(\"\\n--- User Clustering (Dask-ML) ---\")\n",
    "\n",
    "# 1. ì·¨í–¥ ë²¡í„° (pd.crosstab ëŒ€ì²´ -> pivot_table)\n",
    "# crosstab(normalize='index')ëŠ” Daskë¡œ ë§¤ìš° ë³µì¡í•¨.\n",
    "# (ëŒ€ì•ˆ) Dask pivot_tableë¡œ í•©ê³„(sum)ë¥¼ êµ¬í•˜ê³ , map_partitionsë¡œ ì •ê·œí™”(normalize)\n",
    "style_affinity_pivot = df_full.pivot_table(\n",
    "    index='username', columns='style_group', values='score', aggfunc='count'\n",
    ").fillna(0)\n",
    "# (ì •ê·œí™”) ê° í–‰(ìœ ì €)ì˜ í•©ìœ¼ë¡œ ë‚˜ëˆ”\n",
    "user_style_affinity = style_affinity_pivot.map_partitions(\n",
    "    lambda df: df.div(df.sum(axis=1), axis=0),\n",
    "    meta=style_affinity_pivot._meta\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "# 2. í–‰ë™ ê¸°ë°˜ í”¼ì²˜\n",
    "user_numeric_features = df_full.groupby('username').agg(\n",
    "    user_avg_score=('score', 'mean'),\n",
    "    user_avg_abv=('abv', 'mean'),\n",
    "    user_avg_smell=('smell', 'mean')\n",
    ").fillna(0)\n",
    "\n",
    "# 3. (â˜…ìˆ˜ì •â˜…) Dask Merge (pd.concat ëŒ€ì²´)\n",
    "# (SVD ì…€ì´ ì„±ê³µí–ˆë‹¤ëŠ” ê°€ì • í•˜ì— user_latent_df ê²°í•©)\n",
    "if 'user_latent_df' in locals():\n",
    "    user_profile_df = dd.merge(user_numeric_features, user_style_affinity, on='username', how='outer')\n",
    "    user_profile_df = dd.merge(user_profile_df, user_latent_df, on='username', how='left').fillna(0)\n",
    "    print(\"User profileì— Latent Features ê²°í•© (Lazy).\")\n",
    "else:\n",
    "    user_profile_df = dd.merge(user_numeric_features, user_style_affinity, on='username', how='outer').fillna(0)\n",
    "\n",
    "# 4. Dask-ML StandardScaler & KMeans\n",
    "scaler_user = dpr.StandardScaler()\n",
    "user_features_processed = scaler_user.fit_transform(user_profile_df.drop(columns=['username']))\n",
    "\n",
    "kmeans_user = dkm.KMeans(n_clusters=K_USERS, random_state=42, n_init=10)\n",
    "print(\"User K-Means í•™ìŠµ ì‹œì‘ (Dask)...\")\n",
    "\n",
    "# (â˜…ê³„ì‚° ë°œìƒâ˜…) fit_predict ìˆ˜í–‰\n",
    "user_profile_df['user_cluster'] = kmeans_user.fit_predict(user_features_processed)\n",
    "\n",
    "print(f\"User í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ. {K_USERS}ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜ë¨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37336478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MODIFIED CELL 8) - Dask ìµœì¢… í•™ìŠµ ë°ì´í„° ìƒì„±\n",
    "\n",
    "print(\"\\n--- ìµœì¢… í•™ìŠµ ë°ì´í„° ìƒì„± (Dask Merge) ---\")\n",
    "\n",
    "# 1. df_full + beer_cluster\n",
    "df_model = dd.merge(df_full, beer_features_df[['beer_id', 'beer_cluster']], on='beer_id', how='left')\n",
    "\n",
    "# 2. df_model + user_profile\n",
    "user_cols_to_merge = ['username', 'user_cluster', 'user_avg_score']\n",
    "user_latent_cols = [f'user_latent_{i}' for i in range(N_COMPONENTS)]\n",
    "if 'user_latent_df' in locals():\n",
    "    user_cols_to_merge.extend(user_latent_cols)\n",
    "\n",
    "df_model = dd.merge(df_model, \n",
    "                    user_profile_df[user_cols_to_merge], \n",
    "                    on='username', \n",
    "                    how='left')\n",
    "    \n",
    "# 3. df_model + beer_latent\n",
    "if 'beer_latent_df' in locals():\n",
    "    df_model = dd.merge(df_model, beer_latent_df, on='beer_id', how='left')\n",
    "\n",
    "# 4. Target ë³€ìˆ˜ ìƒì„±\n",
    "df_model['is_top_pick'] = (df_model['score'] > (df_model['user_avg_score'] + THRESHOLD)).astype(int)\n",
    "\n",
    "print(\"'is_top_pick' íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ (Lazy).\")\n",
    "\n",
    "# 5. (ë©”ëª¨ë¦¬ í•´ì œ) DaskëŠ” ì¤‘ê°„ ë³€ìˆ˜ë“¤ì„ .persist()ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "# del df_full, user_profile_df, beer_features_df # (í•„ìš”ì‹œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MODIFIED CELL 9) - Dask Train/Test ë¶„ë¦¬\n",
    "\n",
    "print(\"\\n--- Train/Test/Validation ë¶„ë¦¬ (Dask-ML) ---\")\n",
    "\n",
    "features_to_use = [\n",
    "    'smell', 'taste', 'feel',\n",
    "    'style_group', 'geo_group',\n",
    "    'beer_cluster', 'user_cluster'\n",
    "]\n",
    "if 'user_latent_df' in locals():\n",
    "    features_to_use.extend([f'user_latent_{i}' for i in range(N_COMPONENTS)])\n",
    "    features_to_use.extend([f'beer_latent_{i}' for i in range(N_COMPONENTS)])\n",
    "\n",
    "target = 'is_top_pick'\n",
    "\n",
    "# 1. Dask-ML LabelEncoder (Workaround)\n",
    "# (Dask-MLì—ëŠ” LabelEncoderê°€ ì—†ìœ¼ë¯€ë¡œ, .categorize().get_dummies() ë˜ëŠ” .map_partitions ì‚¬ìš©)\n",
    "# ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ .categorize()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "categorical_features_final = ['style_group', 'geo_group', 'beer_cluster', 'user_cluster']\n",
    "df_model = df_model.categorize(columns=categorical_features_final)\n",
    "for col in categorical_features_final:\n",
    "    df_model[col] = df_model[col].cat.codes # .cat.codesë¡œ ìˆ«ìí˜• ë³€í™˜\n",
    "\n",
    "# 2. ê²°ì¸¡ì¹˜ ì œê±°\n",
    "all_cols_needed = features_to_use + [target]\n",
    "df_model = df_model.dropna(subset=all_cols_needed)\n",
    "print(f\"ê²°ì¸¡ì¹˜ ì œê±° (Lazy).\")\n",
    "\n",
    "X = df_model[features_to_use]\n",
    "y = df_model[target]\n",
    "\n",
    "# 3. Dask-ML Train/Test Split\n",
    "X_train, X_test, y_train, y_test = dms.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_sub, X_val, y_train_sub, y_val = dms.train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. (â˜…ì¤‘ìš”â˜…) ë¶„ë¦¬ëœ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— .persist()\n",
    "# XGBoost í•™ìŠµ ì‹œ ë°ì´í„°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì½ì–´ì•¼ í•˜ë¯€ë¡œ, ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€ì‹œí‚µë‹ˆë‹¤.\n",
    "X_train_sub = X_train_sub.persist()\n",
    "y_train_sub = y_train_sub.persist()\n",
    "X_val = X_val.persist()\n",
    "y_val = y_val.persist()\n",
    "X_test = X_test.persist()\n",
    "y_test = y_test.persist()\n",
    "\n",
    "print(f\"Train/Validation/Test ë¶„ë¦¬ ë° Persist ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04dbd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MODIFIED CELL 10) - XGBoost í•™ìŠµ (Dask-XGBoost)\n",
    "\n",
    "print(\"\\n--- XGBoost Hybrid ëª¨ë¸ í•™ìŠµ ì‹œì‘ (Dask) ---\")\n",
    "\n",
    "# 1. ë¶ˆê· í˜• ë°ì´í„° ë¹„ìœ¨ ê³„ì‚°\n",
    "# (â˜…ê³„ì‚° ë°œìƒâ˜…) .compute()ë¡œ ì‹¤ì œ ê°’ì„ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.\n",
    "print(\"í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚° ì¤‘...\")\n",
    "y_train_sub_computed = y_train_sub.compute() # (ì‹œê°„ ì†Œìš”)\n",
    "ratio = (y_train_sub_computed == 0).sum() / (y_train_sub_computed == 1).sum()\n",
    "print(f\"scale_pos_weight ratio: {ratio:.2f}\")\n",
    "\n",
    "# 2. Dask XGBoost ëª¨ë¸ ì„ ì–¸\n",
    "# Dask í´ë¼ì´ì–¸íŠ¸ë¥¼ ëª¨ë¸ì— ì•Œë ¤ì¤˜ì•¼ í•©ë‹ˆë‹¤.\n",
    "dask_model_xgb = xgb.dask.DaskXGBClassifier(\n",
    "    client=client,\n",
    "    objective='binary:logistic', eval_metric='auc',\n",
    "    scale_pos_weight=ratio,\n",
    "    n_estimators=1000, learning_rate=0.05, max_depth=6,\n",
    "    random_state=42,\n",
    "    tree_method='hist' # DaskëŠ” 'hist'ë§Œ ì§€ì›\n",
    ")\n",
    "    \n",
    "# 3. Dask ë°ì´í„°ë¡œ í•™ìŠµ\n",
    "print(\"Dask XGBoost í•™ìŠµ ì‹œì‘...\")\n",
    "dask_model_xgb.fit(\n",
    "    X_train_sub, y_train_sub,\n",
    "    eval_set=[(X_val, y_val)],      \n",
    "    early_stopping_rounds=50,      \n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "print(\"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MODIFIED CELL 11) - ëª¨ë¸ í‰ê°€ (Dask)\n",
    "\n",
    "print(\"\\n--- ëª¨ë¸ í‰ê°€ (Test Set) ---\")\n",
    "\n",
    "# 1. Daskë¡œ ì˜ˆì¸¡ (Lazy)\n",
    "preds_proba_dask = dask_model_xgb.predict_proba(X_test)\n",
    "preds_binary_dask = dask_model_xgb.predict(X_test)\n",
    "\n",
    "# 2. (â˜…ê³„ì‚° ë°œìƒâ˜…) .compute()ë¡œ ì‹¤ì œ ê²°ê³¼(Numpy/Pandas)ë¥¼ ê°€ì ¸ì˜´\n",
    "print(\"Test Set ì˜ˆì¸¡ ê²°ê³¼ ê³„ì‚° ì¤‘...\")\n",
    "preds_proba = preds_proba_dask[:, 1].compute() # 1 í´ë˜ìŠ¤ í™•ë¥ \n",
    "preds_binary = preds_binary_dask.compute()\n",
    "y_test_computed = y_test.compute()\n",
    "print(\"ê³„ì‚° ì™„ë£Œ.\")\n",
    "\n",
    "# 3. Sklearn ì§€í‘œë¡œ í‰ê°€\n",
    "auc_score = roc_auc_score(y_test_computed, preds_proba)\n",
    "\n",
    "print(f\"\\n[Hybrid Model - Test Set ê²°ê³¼]\")\n",
    "print(f\"AUC (Area Under Curve): {auc_score:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_computed, preds_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MODIFIED CELL 12) - ë­í‚¹ ì§€í‘œ í‰ê°€ (P@K, R@K)\n",
    "\n",
    "print(\"\\n--- ë­í‚¹ í‰ê°€ ì§€í‘œ (Test Set) ---\")\n",
    "\n",
    "# 1. Daskì—ì„œ 'username' ì»¬ëŸ¼ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ Persistë¨)\n",
    "usernames_test = X_test['username'].compute()\n",
    "\n",
    "# 2. Pandas DataFrameìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ê²°í•©\n",
    "test_results_df = pd.DataFrame({\n",
    "    'username': usernames_test,\n",
    "    'is_top_pick_true': y_test_computed,\n",
    "    'probability_top_pick': preds_proba\n",
    "})\n",
    "\n",
    "print(\"P@K ê³„ì‚° ì¤€ë¹„ ì™„ë£Œ:\")\n",
    "print(test_results_df.head())\n",
    "\n",
    "# 3. (ê¸°ì¡´ P@K, R@K ê³„ì‚° í•¨ìˆ˜ ì‚¬ìš© - ë™ì¼)\n",
    "def calculate_ranking_metrics(df_results, k=10):\n",
    "    # (ê¸°ì¡´ê³¼ ë™ì¼í•œ í•¨ìˆ˜ ë‚´ìš©)\n",
    "    # ...\n",
    "    user_groups = df_results.groupby('username')\n",
    "    user_metrics = {'precision@k': [], 'recall@k': [], 'map@k': []}\n",
    "    \n",
    "    for username, group in user_groups:\n",
    "        total_true_positives = group['is_top_pick_true'].sum()\n",
    "        if total_true_positives == 0:\n",
    "            continue\n",
    "        top_k_list = group.sort_values('probability_top_pick', ascending=False).head(k)\n",
    "        hits_df = top_k_list[top_k_list['is_top_pick_true'] == 1]\n",
    "        num_hits = len(hits_df)\n",
    "        \n",
    "        precision_at_k = num_hits / k\n",
    "        recall_at_k = num_hits / total_true_positives\n",
    "        user_metrics['precision@k'].append(precision_at_k)\n",
    "        user_metrics['recall@k'].append(recall_at_k)\n",
    "        \n",
    "        if num_hits > 0:\n",
    "            hit_ranks = (top_k_list.reset_index(drop=True).index + 1)[top_k_list['is_top_pick_true'] == 1]\n",
    "            ap_sum = (pd.Series(range(1, num_hits + 1)) / hit_ranks).sum()\n",
    "            average_precision = ap_sum / num_hits \n",
    "            user_metrics['map@k'].append(average_precision)\n",
    "        else:\n",
    "            user_metrics['map@k'].append(0.0)\n",
    "            \n",
    "    if not user_metrics['precision@k']:\n",
    "        return pd.Series(index=['precision@k', 'recall@k', 'map@k'], data=[0.0, 0.0, 0.0])\n",
    "        \n",
    "    return pd.Series(user_metrics).apply(np.mean)\n",
    "\n",
    "# 4. K=5, K=10ì¼ ë•Œì˜ ë­í‚¹ ì§€í‘œ ê³„ì‚°\n",
    "k_5_metrics = calculate_ranking_metrics(test_results_df, k=5)\n",
    "print(f\"\\n[Metrics @ K=5]\\n{k_5_metrics}\")\n",
    "k_10_metrics = calculate_ranking_metrics(test_results_df, k=10)\n",
    "print(f\"\\n[Metrics @ K=10]\\n{k_10_metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
