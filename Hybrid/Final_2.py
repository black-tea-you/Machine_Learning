#!/usr/bin/env python
# coding: utf-8

# In[70]:


# (MODIFIED CELL 2) - Dask Import & Setup

import dask.dataframe as dd
import dask.array as da
import dask_ml.cluster as dkm
import dask_ml.decomposition as dkd
import dask_ml.preprocessing as dpr
import dask_ml.model_selection as dms
from dask.distributed import Client, LocalCluster
from implicit.als import AlternatingLeastSquares

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings

import xgboost as xgb # Dask-XGBoostëŠ” dask.dataframeì„ ì§ì ‘ ë°›ìŒ
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.preprocessing import LabelEncoder # LabelEncoderëŠ” Dask-MLì— ì—†ìŒ (ëŒ€ì²´ í•„ìš”)

# --- 0. í•˜ì´í¼íŒŒë¼ë¯¸í„° (Daskìš©) ---
# (SAMPLE_FRACì€ 1.0ì´ë¯€ë¡œ ì‚­ì œ)
SOURCE_FILE = 'df_master_preprocessed.parquet'
K_BEERS = 6
K_USERS = 8
N_COMPONENTS = 10 # Latent Factor ê°œìˆ˜
THRESHOLD = 0.5   # (ì´ì „ì— ì¡°ì •í•œ ê°’)

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings('ignore')

print("Dask ë° Dask-ML ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ.")


# # (MODIFIED CELL 2) - Dask Client Start

# In[52]:


# (MODIFIED CELL 2) - Dask Client Start

import dask.dataframe as dd
from dask.distributed import Client, LocalCluster, wait # (â˜…â˜…â˜…â˜…â˜… wait ì„í¬íŠ¸ ì¶”ê°€ â˜…â˜…â˜…â˜…â˜…)
import warnings

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings('ignore')

# Dask í´ëŸ¬ìŠ¤í„°ë¥¼ ë¡œì»¬ ë¨¸ì‹ ì— ì„¤ì •í•©ë‹ˆë‹¤.
# (n_workers, memory_limit ë“±ì€ ë¨¸ì‹  ì‚¬ì–‘ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”)
try:
    cluster = LocalCluster(n_workers=4, threads_per_worker=1, memory_limit='16GB')
    client = Client(cluster)
    print("Dask Clientê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"Dask ëŒ€ì‹œë³´ë“œ ë§í¬: {client.dashboard_link}")
    print("ìœ„ ë§í¬ë¥¼ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ Dask ì‘ì—… ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.")
except Exception as e:
    print(f"Dask í´ë¼ì´ì–¸íŠ¸ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")
    print("Daskê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
SOURCE_FILE = 'df_master_preprocessed.parquet'


# # (MODIFIED CELL 3) - 840ë§Œ í–‰ Daskë¡œ ë¡œë“œ (ìƒ˜í”Œë§ X)

# In[64]:


# (MODIFIED CELL 3) - 840ë§Œ í–‰ Daskë¡œ ë¡œë“œ (ìƒ˜í”Œë§ X)

print(f"--- 1. ì „ì²´ ë°ì´í„° ë¡œë“œ ({SOURCE_FILE}) ---")
# (â˜…ì¤‘ìš”â˜…) 840ë§Œ í–‰ ì „ì²´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (ì•„ì§ ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°€ì§€ ì•ŠìŒ)
# engine='pyarrow'ê°€ parquet ì²˜ë¦¬ì— íš¨ìœ¨ì ì…ë‹ˆë‹¤.
# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì§€ì •í•˜ë©´ ì†ë„ê°€ ë” ë¹¨ë¼ì§‘ë‹ˆë‹¤.
cols_to_load = ['date', 'style', 'country_brewery', 'abv', 'smell', 'taste', 
                'feel', 'score', 'username', 'beer_id']

try:
    df_full = dd.read_parquet(
        SOURCE_FILE, 
        engine='pyarrow',
        columns=cols_to_load 
    )
    
    # npartitionsë¥¼ ì¡°ì •í•˜ì—¬ ì²­í¬ í¬ê¸° ë³€ê²½ ê°€ëŠ¥ (ì˜ˆ: repartition(npartitions=50))
    # df_full = df_full.repartition(npartitions=50) # (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)

    # Daskê°€ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ë‘ê³  ì¬ì‚¬ìš©í•˜ë„ë¡ ì§€ì‹œ (ê³„ì‚° ì†ë„ í–¥ìƒ)
    df_full = df_full.persist() 
    
    print(f"ì „ì²´ ë°ì´í„° ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ. (Dask lazy loading)")
    print(f"ì „ì²´ ë°ì´í„° í–‰ ìˆ˜ (ì˜ˆìƒ): {len(df_full)}") # .compute() ì „ì´ë¼ë„ len()ì€ ë¹ ë¥´ê²Œ ê³„ì‚°ë¨

except Exception as e:
    print(f"Daskë¡œ Parquet ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("Dask ë˜ëŠ” pyarrow ì„¤ì¹˜, íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")


# # (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§

# In[65]:


# (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§

print("\n--- 'style_group' ë° 'geo_group' í”¼ì²˜ ìƒì„± ì¤‘ (Dask) ---")

def group_style(style):
    if 'IPA' in str(style): return 'IPA'
    if 'Stout' in str(style): return 'Stout'
    if 'Ale' in str(style): return 'Ale'
    return 'Other'
    
def group_country(country):
    if country == 'US': return 'US'
    if country in ['DE', 'GB', 'BE']: return 'Europe'
    return 'Other'

# (â˜…ìˆ˜ì •â˜…) .apply -> .map_partitions
# DaskëŠ” map_partitionsë¥¼ ì‚¬ìš©í•  ë•Œ ì¶œë ¥ ê²°ê³¼ì˜ íƒ€ì…(meta)ì„ ì•Œë ¤ì¤˜ì•¼ í•©ë‹ˆë‹¤.
df_full['style_group'] = df_full['style'].map_partitions(
    lambda s: s.apply(group_style), 
    meta=pd.Series(dtype='object', name='style_group')
)
df_full['geo_group'] = df_full['country_brewery'].map_partitions(
    lambda s: s.apply(group_country), 
    meta=pd.Series(dtype='object', name='geo_group')
)

print("'style_group', 'geo_group' ìƒì„± ì™„ë£Œ (Lazy).")


# # (MODIFIED CELL 5) - Dask Load Test (wait í•¨ìˆ˜ ìˆ˜ì •)

# In[66]:


# (MODIFIED CELL 5) - Dask Load Test (wait í•¨ìˆ˜ ìˆ˜ì •)

import time 

print(f"--- 1. Daskë¡œ ì „ì²´ ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸ ({SOURCE_FILE}) ---")

if 'client' not in locals():
    print("!!! ì˜¤ë¥˜: Dask Clientê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì´ì „ ì…€ ì‹¤í–‰ í•„ìš”)")
else:
    try:
        # 1. 840ë§Œ í–‰ ì „ì²´ë¥¼ Daskë¡œ ë¡œë“œ (Lazy)
        cols_to_load = ['username', 'beer_id', 'score', 'date', 'style', 'country_brewery']
        
        df_full = dd.read_parquet(
            SOURCE_FILE, 
            engine='pyarrow',
            columns=cols_to_load
        )
        
        print(f"íŒŒì¼ ì½ê¸° ì¤€ë¹„ ì™„ë£Œ. (Lazy Loading)")
        
        # 2. (â˜…í•µì‹¬ í…ŒìŠ¤íŠ¸â˜…) .persist()
        print("Dask .persist() ì‹œì‘... (ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤. ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")
        start_time = time.time()
        
        df_full = df_full.persist() # (â˜…ê³„ì‚° ë°œìƒâ˜…)
        
        # 3. (â˜…â˜…â˜…â˜…â˜… ì˜¤ë¥˜ ìˆ˜ì • â˜…â˜…â˜…â˜…â˜…)
        # .persist()ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸° (client.wait_for_futures -> wait)
        wait(df_full) 
        
        total_rows = len(df_full)
        
        print(f"--- ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ---")
        print(f"ë¡œë“œ ì‹œê°„: {(time.time() - start_time):.2f} ì´ˆ")
        print(f"ì´ {total_rows} í–‰ì´ Dask ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except FileNotFoundError:
        print(f"!!! ì˜¤ë¥˜: '{SOURCE_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"!!! Dask ë¡œë“œ(.persist()) ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ì´ì œ 'time' ë˜ëŠ” 'wait' ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¼ë©´, ì´ê²ƒì´ ì§„ì§œ ë©”ëª¨ë¦¬ ë¶€ì¡±/pyarrow ì˜¤ë¥˜ì…ë‹ˆë‹¤.")


# # (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§

# In[67]:


# (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§

print("\n--- 'style_group' ë° 'geo_group' í”¼ì²˜ ìƒì„± ì¤‘ (Dask) ---")

# (df_fullì´ ì´ì „ Cell 5 í…ŒìŠ¤íŠ¸ì—ì„œ ë¡œë“œë˜ì—ˆë‹¤ê³  ê°€ì •)
if 'df_full' not in locals() or not isinstance(df_full, dd.DataFrame):
    print("!!! ì˜¤ë¥˜: df_full (Dask DataFrame)ì´ ì—†ìŠµë‹ˆë‹¤. ë¡œë“œ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
else:
    def group_style(style):
        # (styleì€ NAê°€ ìˆì–´ë„ 'IPA' in 'NA' (False)ê°€ ë˜ì–´ ì•ˆì „í•¨)
        if 'IPA' in str(style): return 'IPA'
        if 'Stout' in str(style): return 'Stout'
        if 'Ale' in str(style): return 'Ale'
        return 'Other'
        
    def group_country(country):
        # --- ğŸ’¡ [ìˆ˜ì •ëœ ë¶€ë¶„] ---
        # "boolean value of NA" ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ NAë¥¼ ë¨¼ì € ì²´í¬í•©ë‹ˆë‹¤.
        if pd.isna(country):
            return 'Other' # ë˜ëŠ” return pd.NA
        # ---
        if country == 'US': return 'US'
        if country in ['DE', 'GB', 'BE']: return 'Europe'
        return 'Other'

    # (â˜…ìˆ˜ì •â˜…) .apply -> .map_partitions
    df_full['style_group'] = df_full['style'].map_partitions(
        lambda s: s.apply(group_style), 
        meta=pd.Series(dtype='object', name='style_group')
    )
    df_full['geo_group'] = df_full['country_brewery'].map_partitions(
        lambda s: s.apply(group_country), 
        meta=pd.Series(dtype='object', name='geo_group')
    )
    
    # (â˜…í•µì‹¬â˜…) .persist()ë¡œ CB í”¼ì²˜ ê³„ì‚° ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€
    df_full = df_full.persist()
    wait(df_full) # ê³„ì‚° ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
    
    print("'style_group', 'geo_group' ìƒì„± ë° Persist ì™„ë£Œ.")


# # (MODIFIED CELL 5) - Latent Feature (SVD) (Dask)

# In[60]:


# --- 3-A. Latent Feature ì¶”ì¶œ (SVD) (Dask) ---
print("--- 3-A. Latent Feature ì¶”ì¶œ (SVD) (Dask) ---")

try:
    # --- 1. Dask DataFrame ì¤€ë¹„ ---
    if 'df_full' not in locals():
         raise NameError("df_full (Dask DataFrame)ì´ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ë¡œë“œ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    # df_fullì„ ddf ë³€ìˆ˜ëª…ìœ¼ë¡œ ë°›ì•„ì„œ SVD ì „ìš©ìœ¼ë¡œ ì‚¬ìš©
    ddf = df_full 
    
    # --- 2. NA/ê²°ì¸¡ì¹˜ ì œê±° ---
    # í”¼ë²—ì— ì‚¬ìš©í•  í•µì‹¬ ì»¬ëŸ¼ 3ê°œ('username', 'beer_id', 'score')ì˜ NAë¥¼ ì œê±°
    print("í”¼ë²— ëŒ€ìƒ ì»¬ëŸ¼('username', 'beer_id', 'score')ì˜ ê²°ì¸¡ì¹˜(NA)ë¥¼ ì œê±°í•©ë‹ˆë‹¤...")
    ddf = ddf.dropna(subset=['username', 'beer_id', 'score'])
    print("ê²°ì¸¡ì¹˜ ì œê±° ì™„ë£Œ (Lazy).")

    # --- 3. categorize() ì‹¤í–‰ ---
    print("Warning: 840ë§Œ í–‰ì— ëŒ€í•œ categorize() ë° pivot_tableì€")
    print("         ë§¤ìš° ëŠë¦¬ê³  Dask í´ëŸ¬ìŠ¤í„° ë©”ëª¨ë¦¬ë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    print("Pivotingì„ ìœ„í•´ 'username'ê³¼ 'beer_id'ë¥¼ category íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤...")
    print("ì´ ì‘ì—…ì€ ê³ ìœ ê°’(cardinality)ì´ ë§ì•„ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    ddf = ddf.categorize(columns=['username', 'beer_id'])
    
    # (ì„ íƒ ì‚¬í•­) ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë‹¤ë©´ persist() ì‚¬ìš©
    # ddf = ddf.persist() 

    print("Category íƒ€ì… ë³€í™˜ ì™„ë£Œ (Known). Pivot ì‹œì‘...")

    # --- 4. pivot_table ì‹¤í–‰ ('score' ì‚¬ìš©) ---
    pivot_df = ddf.pivot_table(index='username', 
                               columns='beer_id', 
                               values='score') 

    # --- 5. SVDë¥¼ ìœ„í•œ .fillna(0) ---
    print("Pivot í…Œì´ë¸” ìƒì„± ì™„ë£Œ. SVDë¥¼ ìœ„í•´ NaNì„ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤...")
    pivot_df_filled = pivot_df.fillna(0)
    print("NaN ì±„ìš°ê¸° ì™„ë£Œ(Lazy). SVD ê³„ì‚° ì‹œì‘...")
    
    # --- ğŸ’¡ 6. [ìˆ˜ì •ëœ ë¶€ë¶„] SVD ë¡œì§ ì‹¤í–‰ ---
    # (N_COMPONENTSëŠ” Cell 37ì—ì„œ 10ìœ¼ë¡œ ì •ì˜ë¨)
    svd = dkd.TruncatedSVD(n_components=N_COMPONENTS, random_state=42)
    
    print("SVD .fit_transform() ê³„ì‚° ì‹œì‘ (Dask Array ì‚¬ìš©)...")
    
    # 
    # (ğŸ’¡ğŸ’¡ğŸ’¡ ì—¬ê¸°ê°€ ìˆ˜ì •ëœ ë¶€ë¶„ì…ë‹ˆë‹¤ ğŸ’¡ğŸ’¡ğŸ’¡)
    # 'numblocks' ì˜¤ë¥˜ í•´ê²°: .valuesë¥¼ ë¶™ì—¬ Dask Arrayë¡œ ë³€í™˜
    #
    user_latent_features = svd.fit_transform(pivot_df_filled.values) # <--- .values ì¶”ê°€
    beer_latent_features = svd.components_
    
    # (SVD ê²°ê³¼ë¥¼ ë‹¤ë¥¸ ì…€ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ DataFrameìœ¼ë¡œ ë³€í™˜)
    user_latent_df = pivot_df_filled.index.to_frame(name='username')\
                       .assign(**{f'user_latent_{i}': user_latent_features[:, i] for i in range(N_COMPONENTS)})
    beer_latent_df = pivot_df_filled.columns.to_frame(name='beer_id')\
                       .assign(**{f'beer_latent_{i}': beer_latent_features[i, :] for i in range(N_COMPONENTS)})

    # (ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ê³ ì •)
    user_latent_df = user_latent_df.persist()
    beer_latent_df = beer_latent_df.persist()
    wait(user_latent_df)
    wait(beer_latent_df)
    
    print("--- Dask SVD/Pivot ëª¨ë“  ì‘ì—… ì™„ë£Œ ---")


except Exception as e:
    print(f"!!! Dask SVD/Pivot ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("--- 'category dtype' ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¼ë©´, ì´ê²ƒì´ ì§„ì§œ ë©”ëª¨ë¦¬/ì„±ëŠ¥ í•œê³„ì…ë‹ˆë‹¤. ---")


# # (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§???????

# In[36]:


# (MODIFIED CELL 4) - Dask í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§

print("\n--- 'style_group' ë° 'geo_group' í”¼ì²˜ ìƒì„± ì¤‘ (Dask) ---")

# (df_fullì´ ì´ì „ Cell 5 í…ŒìŠ¤íŠ¸ì—ì„œ ë¡œë“œë˜ì—ˆë‹¤ê³  ê°€ì •)
if 'df_full' not in locals() or not isinstance(df_full, dd.DataFrame):
    print("!!! ì˜¤ë¥˜: df_full (Dask DataFrame)ì´ ì—†ìŠµë‹ˆë‹¤. ë¡œë“œ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
else:
    def group_style(style):
        if 'IPA' in str(style): return 'IPA'
        if 'Stout' in str(style): return 'Stout'
        if 'Ale' in str(style): return 'Ale'
        return 'Other'
        
    def group_country(country):
        if country == 'US': return 'US'
        if country in ['DE', 'GB', 'BE']: return 'Europe'
        return 'Other'

    # (â˜…ìˆ˜ì •â˜…) .apply -> .map_partitions
    df_full['style_group'] = df_full['style'].map_partitions(
        lambda s: s.apply(group_style), 
        meta=pd.Series(dtype='object', name='style_group')
    )
    df_full['geo_group'] = df_full['country_brewery'].map_partitions(
        lambda s: s.apply(group_country), 
        meta=pd.Series(dtype='object', name='geo_group')
    )
    
    # (â˜…í•µì‹¬â˜…) .persist()ë¡œ CB í”¼ì²˜ ê³„ì‚° ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€
    df_full = df_full.persist()
    wait(df_full) # ê³„ì‚° ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
    
    print("'style_group', 'geo_group' ìƒì„± ë° Persist ì™„ë£Œ.")


# In[29]:


# (MODIFIED CELL 6) - Beer Clustering (Dask-ML)

print("\n--- Beer Clustering (Dask-ML) ---")

# 1. ì¬ë£Œ ì¤€ë¹„
beer_features_df = df_full[['beer_id', 'style_group', 'geo_group']]\
    .drop_duplicates(subset=['beer_id'])\
    .dropna()\
    .set_index('beer_id') # set_indexëŠ” Daskì—ì„œ ëŠë¦° ì‘ì—…

# 2. Dask-ML OneHotEncoding (pd.get_dummies ëŒ€ì²´)
# (Dask-MLì€ ë¬¸ìì—´ ì¹´í…Œê³ ë¦¬ë¥¼ ìˆ«ìë¡œ ë¨¼ì € ë³€í™˜í•´ì•¼ í•  ìˆ˜ ìˆìŒ)
# (ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ get_dummiesë¥¼ ì“°ì§€ë§Œ, ëŒ€ìš©ëŸ‰ì—ì„œëŠ” Categorizer + OneHotEncoder ì¶”ì²œ)
beer_features_processed = dd.get_dummies(
    beer_features_df.categorize(columns=['style_group', 'geo_group']),
    columns=['style_group', 'geo_group']
)

# 3. Dask-ML K-Means
kmeans_beer = dkm.KMeans(n_clusters=K_BEERS, random_state=42, n_init=10)
print("Beer K-Means í•™ìŠµ ì‹œì‘ (Dask)...")

# (â˜…ê³„ì‚° ë°œìƒâ˜…) .fit_predict()ëŠ” ê³„ì‚°ì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤ (ë©”ëª¨ë¦¬ ì£¼ì˜)
beer_features_df['beer_cluster'] = kmeans_beer.fit_predict(beer_features_processed)
beer_features_df = beer_features_df.reset_index() # Mergeë¥¼ ìœ„í•´ ì¸ë±ìŠ¤ ë¦¬ì…‹

print(f"Beer í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ. {K_BEERS}ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜ë¨.")


# In[ ]:


# (MODIFIED CELL 7) - User Clustering (Dask-ML)

print("\n--- User Clustering (Dask-ML) ---")

# 1. ì·¨í–¥ ë²¡í„° (pd.crosstab ëŒ€ì²´ -> pivot_table)
# crosstab(normalize='index')ëŠ” Daskë¡œ ë§¤ìš° ë³µì¡í•¨.
# (ëŒ€ì•ˆ) Dask pivot_tableë¡œ í•©ê³„(sum)ë¥¼ êµ¬í•˜ê³ , map_partitionsë¡œ ì •ê·œí™”(normalize)
style_affinity_pivot = df_full.pivot_table(
    index='username', columns='style_group', values='score', aggfunc='count'
).fillna(0)
# (ì •ê·œí™”) ê° í–‰(ìœ ì €)ì˜ í•©ìœ¼ë¡œ ë‚˜ëˆ”
user_style_affinity = style_affinity_pivot.map_partitions(
    lambda df: df.div(df.sum(axis=1), axis=0),
    meta=style_affinity_pivot._meta
).fillna(0)


# 2. í–‰ë™ ê¸°ë°˜ í”¼ì²˜
user_numeric_features = df_full.groupby('username').agg(
    user_avg_score=('score', 'mean'),
    user_avg_abv=('abv', 'mean'),
    user_avg_smell=('smell', 'mean')
).fillna(0)

# 3. (â˜…ìˆ˜ì •â˜…) Dask Merge (pd.concat ëŒ€ì²´)
# (SVD ì…€ì´ ì„±ê³µí–ˆë‹¤ëŠ” ê°€ì • í•˜ì— user_latent_df ê²°í•©)
if 'user_latent_df' in locals():
    user_profile_df = dd.merge(user_numeric_features, user_style_affinity, on='username', how='outer')
    user_profile_df = dd.merge(user_profile_df, user_latent_df, on='username', how='left').fillna(0)
    print("User profileì— Latent Features ê²°í•© (Lazy).")
else:
    user_profile_df = dd.merge(user_numeric_features, user_style_affinity, on='username', how='outer').fillna(0)

# 4. Dask-ML StandardScaler & KMeans
scaler_user = dpr.StandardScaler()
user_features_processed = scaler_user.fit_transform(user_profile_df.drop(columns=['username']))

kmeans_user = dkm.KMeans(n_clusters=K_USERS, random_state=42, n_init=10)
print("User K-Means í•™ìŠµ ì‹œì‘ (Dask)...")

# (â˜…ê³„ì‚° ë°œìƒâ˜…) fit_predict ìˆ˜í–‰
user_profile_df['user_cluster'] = kmeans_user.fit_predict(user_features_processed)

print(f"User í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ. {K_USERS}ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜ë¨.")


# In[ ]:


# (MODIFIED CELL 8) - Dask ìµœì¢… í•™ìŠµ ë°ì´í„° ìƒì„±

print("\n--- ìµœì¢… í•™ìŠµ ë°ì´í„° ìƒì„± (Dask Merge) ---")

# 1. df_full + beer_cluster
df_model = dd.merge(df_full, beer_features_df[['beer_id', 'beer_cluster']], on='beer_id', how='left')

# 2. df_model + user_profile
user_cols_to_merge = ['username', 'user_cluster', 'user_avg_score']
user_latent_cols = [f'user_latent_{i}' for i in range(N_COMPONENTS)]
if 'user_latent_df' in locals():
    user_cols_to_merge.extend(user_latent_cols)

df_model = dd.merge(df_model, 
                    user_profile_df[user_cols_to_merge], 
                    on='username', 
                    how='left')
    
# 3. df_model + beer_latent
if 'beer_latent_df' in locals():
    df_model = dd.merge(df_model, beer_latent_df, on='beer_id', how='left')

# 4. Target ë³€ìˆ˜ ìƒì„±
df_model['is_top_pick'] = (df_model['score'] > (df_model['user_avg_score'] + THRESHOLD)).astype(int)

print("'is_top_pick' íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ (Lazy).")

# 5. (ë©”ëª¨ë¦¬ í•´ì œ) DaskëŠ” ì¤‘ê°„ ë³€ìˆ˜ë“¤ì„ .persist()ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
# del df_full, user_profile_df, beer_features_df # (í•„ìš”ì‹œ)


# In[ ]:


# (MODIFIED CELL 9) - Dask Train/Test ë¶„ë¦¬

print("\n--- Train/Test/Validation ë¶„ë¦¬ (Dask-ML) ---")

features_to_use = [
    'smell', 'taste', 'feel',
    'style_group', 'geo_group',
    'beer_cluster', 'user_cluster'
]
if 'user_latent_df' in locals():
    features_to_use.extend([f'user_latent_{i}' for i in range(N_COMPONENTS)])
    features_to_use.extend([f'beer_latent_{i}' for i in range(N_COMPONENTS)])

target = 'is_top_pick'

# 1. Dask-ML LabelEncoder (Workaround)
# (Dask-MLì—ëŠ” LabelEncoderê°€ ì—†ìœ¼ë¯€ë¡œ, .categorize().get_dummies() ë˜ëŠ” .map_partitions ì‚¬ìš©)
# ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ .categorize()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
categorical_features_final = ['style_group', 'geo_group', 'beer_cluster', 'user_cluster']
df_model = df_model.categorize(columns=categorical_features_final)
for col in categorical_features_final:
    df_model[col] = df_model[col].cat.codes # .cat.codesë¡œ ìˆ«ìí˜• ë³€í™˜

# 2. ê²°ì¸¡ì¹˜ ì œê±°
all_cols_needed = features_to_use + [target]
df_model = df_model.dropna(subset=all_cols_needed)
print(f"ê²°ì¸¡ì¹˜ ì œê±° (Lazy).")

X = df_model[features_to_use]
y = df_model[target]

# 3. Dask-ML Train/Test Split
X_train, X_test, y_train, y_test = dms.train_test_split(X, y, test_size=0.2, random_state=42)
X_train_sub, X_val, y_train_sub, y_val = dms.train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# 4. (â˜…ì¤‘ìš”â˜…) ë¶„ë¦¬ëœ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— .persist()
# XGBoost í•™ìŠµ ì‹œ ë°ì´í„°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì½ì–´ì•¼ í•˜ë¯€ë¡œ, ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€ì‹œí‚µë‹ˆë‹¤.
X_train_sub = X_train_sub.persist()
y_train_sub = y_train_sub.persist()
X_val = X_val.persist()
y_val = y_val.persist()
X_test = X_test.persist()
y_test = y_test.persist()

print(f"Train/Validation/Test ë¶„ë¦¬ ë° Persist ì™„ë£Œ.")


# In[ ]:


# (MODIFIED CELL 10) - XGBoost í•™ìŠµ (Dask-XGBoost)

print("\n--- XGBoost Hybrid ëª¨ë¸ í•™ìŠµ ì‹œì‘ (Dask) ---")

# 1. ë¶ˆê· í˜• ë°ì´í„° ë¹„ìœ¨ ê³„ì‚°
# (â˜…ê³„ì‚° ë°œìƒâ˜…) .compute()ë¡œ ì‹¤ì œ ê°’ì„ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.
print("í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚° ì¤‘...")
y_train_sub_computed = y_train_sub.compute() # (ì‹œê°„ ì†Œìš”)
ratio = (y_train_sub_computed == 0).sum() / (y_train_sub_computed == 1).sum()
print(f"scale_pos_weight ratio: {ratio:.2f}")

# 2. Dask XGBoost ëª¨ë¸ ì„ ì–¸
# Dask í´ë¼ì´ì–¸íŠ¸ë¥¼ ëª¨ë¸ì— ì•Œë ¤ì¤˜ì•¼ í•©ë‹ˆë‹¤.
dask_model_xgb = xgb.dask.DaskXGBClassifier(
    client=client,
    objective='binary:logistic', eval_metric='auc',
    scale_pos_weight=ratio,
    n_estimators=1000, learning_rate=0.05, max_depth=6,
    random_state=42,
    tree_method='hist' # DaskëŠ” 'hist'ë§Œ ì§€ì›
)
    
# 3. Dask ë°ì´í„°ë¡œ í•™ìŠµ
print("Dask XGBoost í•™ìŠµ ì‹œì‘...")
dask_model_xgb.fit(
    X_train_sub, y_train_sub,
    eval_set=[(X_val, y_val)],      
    early_stopping_rounds=50,      
    verbose=100
)

print("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")


# In[ ]:


# (MODIFIED CELL 11) - ëª¨ë¸ í‰ê°€ (Dask)

print("\n--- ëª¨ë¸ í‰ê°€ (Test Set) ---")

# 1. Daskë¡œ ì˜ˆì¸¡ (Lazy)
preds_proba_dask = dask_model_xgb.predict_proba(X_test)
preds_binary_dask = dask_model_xgb.predict(X_test)

# 2. (â˜…ê³„ì‚° ë°œìƒâ˜…) .compute()ë¡œ ì‹¤ì œ ê²°ê³¼(Numpy/Pandas)ë¥¼ ê°€ì ¸ì˜´
print("Test Set ì˜ˆì¸¡ ê²°ê³¼ ê³„ì‚° ì¤‘...")
preds_proba = preds_proba_dask[:, 1].compute() # 1 í´ë˜ìŠ¤ í™•ë¥ 
preds_binary = preds_binary_dask.compute()
y_test_computed = y_test.compute()
print("ê³„ì‚° ì™„ë£Œ.")

# 3. Sklearn ì§€í‘œë¡œ í‰ê°€
auc_score = roc_auc_score(y_test_computed, preds_proba)

print(f"\n[Hybrid Model - Test Set ê²°ê³¼]")
print(f"AUC (Area Under Curve): {auc_score:.4f}")
print("\nClassification Report:")
print(classification_report(y_test_computed, preds_binary))


# In[ ]:


# (MODIFIED CELL 12) - ë­í‚¹ ì§€í‘œ í‰ê°€ (P@K, R@K)

print("\n--- ë­í‚¹ í‰ê°€ ì§€í‘œ (Test Set) ---")

# 1. Daskì—ì„œ 'username' ì»¬ëŸ¼ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ Persistë¨)
usernames_test = X_test['username'].compute()

# 2. Pandas DataFrameìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ê²°í•©
test_results_df = pd.DataFrame({
    'username': usernames_test,
    'is_top_pick_true': y_test_computed,
    'probability_top_pick': preds_proba
})

print("P@K ê³„ì‚° ì¤€ë¹„ ì™„ë£Œ:")
print(test_results_df.head())

# 3. (ê¸°ì¡´ P@K, R@K ê³„ì‚° í•¨ìˆ˜ ì‚¬ìš© - ë™ì¼)
def calculate_ranking_metrics(df_results, k=10):
    # (ê¸°ì¡´ê³¼ ë™ì¼í•œ í•¨ìˆ˜ ë‚´ìš©)
    # ...
    user_groups = df_results.groupby('username')
    user_metrics = {'precision@k': [], 'recall@k': [], 'map@k': []}
    
    for username, group in user_groups:
        total_true_positives = group['is_top_pick_true'].sum()
        if total_true_positives == 0:
            continue
        top_k_list = group.sort_values('probability_top_pick', ascending=False).head(k)
        hits_df = top_k_list[top_k_list['is_top_pick_true'] == 1]
        num_hits = len(hits_df)
        
        precision_at_k = num_hits / k
        recall_at_k = num_hits / total_true_positives
        user_metrics['precision@k'].append(precision_at_k)
        user_metrics['recall@k'].append(recall_at_k)
        
        if num_hits > 0:
            hit_ranks = (top_k_list.reset_index(drop=True).index + 1)[top_k_list['is_top_pick_true'] == 1]
            ap_sum = (pd.Series(range(1, num_hits + 1)) / hit_ranks).sum()
            average_precision = ap_sum / num_hits 
            user_metrics['map@k'].append(average_precision)
        else:
            user_metrics['map@k'].append(0.0)
            
    if not user_metrics['precision@k']:
        return pd.Series(index=['precision@k', 'recall@k', 'map@k'], data=[0.0, 0.0, 0.0])
        
    return pd.Series(user_metrics).apply(np.mean)

# 4. K=5, K=10ì¼ ë•Œì˜ ë­í‚¹ ì§€í‘œ ê³„ì‚°
k_5_metrics = calculate_ranking_metrics(test_results_df, k=5)
print(f"\n[Metrics @ K=5]\n{k_5_metrics}")
k_10_metrics = calculate_ranking_metrics(test_results_df, k=10)
print(f"\n[Metrics @ K=10]\n{k_10_metrics}")

